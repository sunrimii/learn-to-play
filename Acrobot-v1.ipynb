{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Gradient實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(6,)\n",
      "Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.memory = {'observation': [], 'action': [], 'reward': []}\n",
    "        self.gamma = .95 # 對歷史記憶的衰減係數\n",
    "        \n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(6, activation='relu', input_shape=(6, )), \n",
    "            tf.keras.layers.BatchNormalization(), \n",
    "            \n",
    "            tf.keras.layers.Dense(64, activation='relu'), \n",
    "            tf.keras.layers.BatchNormalization(), \n",
    "            tf.keras.layers.Dropout(.1), \n",
    "            \n",
    "            tf.keras.layers.Dense(3, activation='softmax'), \n",
    "        ])\n",
    "        self.model.compile(optimizer='adam', \n",
    "                           loss='CategoricalCrossentropy')\n",
    "    \n",
    "    def get_action(self, observation):\n",
    "        prob = self.model.predict(observation.reshape(1, 6))\n",
    "        action = np.random.choice((-1, 0, 1), p=prob.reshape(3, ))\n",
    "        return action\n",
    "\n",
    "    def remember(self, observation, action, reward):\n",
    "        self.memory['observation'].append(observation)\n",
    "        self.memory['action'].append(action)\n",
    "        self.memory['reward'].append(reward)\n",
    "    \n",
    "    def _encode_onehot(self, action):\n",
    "        if action == -1:\n",
    "            return (1, 0, 0)\n",
    "        elif action == 0:\n",
    "            return (0, 1, 0)\n",
    "        else:\n",
    "            return (0, 0, 1)\n",
    "            \n",
    "    def _preprocess_train_data(self):\n",
    "        self.memory['observation'] = np.array(self.memory['observation'])\n",
    "\n",
    "        self.memory['action'] = map(self._encode_onehot, self.memory['action']) # 獨熱編碼\n",
    "        self.memory['action'] = np.array(list(self.memory['action']))\n",
    "        \n",
    "        self.memory['reward'] = itertools.accumulate(reversed(self.memory['reward']), \n",
    "                                                     lambda x, y: x*self.gamma + y) # 累積期望\n",
    "        self.memory['reward'] = np.array(list(self.memory['reward']))\n",
    "        self.memory['reward'] = (self.memory['reward'] - self.memory['reward'].mean()) \\\n",
    "                                / self.memory['reward'].std() # 標準化\n",
    "        \n",
    "    def train(self):\n",
    "        self._preprocess_train_data()\n",
    "        self.model.fit(self.memory['observation'], \n",
    "                       self.memory['action'], \n",
    "                       batch_size=500, # 相當於不分批量\n",
    "                       sample_weight=self.memory['reward'], \n",
    "                       verbose=0)\n",
    "        \n",
    "        self.memory['observation'] = []\n",
    "        self.memory['action'] = []\n",
    "        self.memory['reward'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, score: -500.0\n",
      "episode: 1, score: -431.0\n",
      "episode: 2, score: -500.0\n",
      "episode: 3, score: -500.0\n",
      "episode: 4, score: -500.0\n",
      "episode: 5, score: -500.0\n",
      "episode: 6, score: -500.0\n",
      "episode: 7, score: -500.0\n",
      "episode: 8, score: -500.0\n",
      "episode: 9, score: -500.0\n",
      "episode: 10, score: -483.0\n",
      "episode: 11, score: -443.0\n",
      "episode: 12, score: -500.0\n",
      "episode: 13, score: -489.0\n",
      "episode: 14, score: -500.0\n",
      "episode: 15, score: -317.0\n",
      "episode: 16, score: -484.0\n",
      "episode: 17, score: -250.0\n",
      "episode: 18, score: -370.0\n",
      "episode: 19, score: -348.0\n",
      "episode: 20, score: -364.0\n",
      "episode: 21, score: -290.0\n",
      "episode: 22, score: -500.0\n",
      "episode: 23, score: -403.0\n",
      "episode: 24, score: -343.0\n",
      "episode: 25, score: -294.0\n",
      "episode: 26, score: -299.0\n",
      "episode: 27, score: -390.0\n",
      "episode: 28, score: -313.0\n",
      "episode: 29, score: -332.0\n",
      "episode: 30, score: -264.0\n",
      "episode: 31, score: -278.0\n",
      "episode: 32, score: -389.0\n",
      "episode: 33, score: -350.0\n",
      "episode: 34, score: -460.0\n",
      "episode: 35, score: -429.0\n",
      "episode: 36, score: -323.0\n",
      "episode: 37, score: -476.0\n",
      "episode: 38, score: -284.0\n",
      "episode: 39, score: -239.0\n",
      "episode: 40, score: -236.0\n",
      "episode: 41, score: -500.0\n",
      "episode: 42, score: -312.0\n",
      "episode: 43, score: -182.0\n",
      "episode: 44, score: -314.0\n",
      "episode: 45, score: -228.0\n",
      "episode: 46, score: -213.0\n",
      "episode: 47, score: -400.0\n",
      "episode: 48, score: -185.0\n",
      "episode: 49, score: -189.0\n"
     ]
    }
   ],
   "source": [
    "# 訓練次數\n",
    "episodes = 50\n",
    "\n",
    "agent = Agent()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    observation = env.reset()\n",
    "    score = 0\n",
    "    while True:\n",
    "        action = agent.get_action(observation)\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        agent.remember(observation, action, reward)\n",
    "        observation = next_observation\n",
    "        score += reward\n",
    "        \n",
    "        if done:\n",
    "            agent.train()\n",
    "            print(f'episode: {episode}, score: {score}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演示學習成果\n",
    "\n",
    "上限500回合\n",
    "\n",
    "分數-100代表在第100回合完成\n",
    "\n",
    "分數-150代表在第150回合完成\n",
    "\n",
    "分數-500代表未完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -349.0\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "score = 0\n",
    "while True:\n",
    "    env.render()\n",
    "    action = agent.get_action(observation)\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    score += reward\n",
    "    time.sleep(0.02)\n",
    "    \n",
    "    if done:\n",
    "        print(f'score: {score}')\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit7c34b89357aa4b6d86f880d1ca073f9f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
